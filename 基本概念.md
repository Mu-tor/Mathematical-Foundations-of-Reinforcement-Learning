# 基本概念


### **State：** The status of the agent with respect to the environment


### **State space：** the set of all states $S=\{s_i\}_{i=1}^9$


### **Action:** For each st ate, there are five possible actions: $a_1, \cdots , a_5$


### **Action space of a state:** 一个状态的所有可能动作的集合


### **State transition：** 当执行一个动作时， 由一个状态转移到另一个状态

$$
s_1 \stackrel{a_2}{\to}s_2
$$


### **Policy:** 告诉agent在当前状态下应该采取什么行动


### **Reward:** agent 采取行动后得到的数

- 一个正数代表鼓励agent进行该行动

- 一个负数代表不希望这样的行为发生



### **Trajectory and return：** 

#### **Trajectory：** 状态-行动-奖励的链

$$
S_1 \mathop{\to} \limits_{r=0}^{a_2} S_2 \mathop{\to} \limits_{r=0}^{a_3} S_5 \mathop{\to} \limits_{r=0}^{a_3} S_8 \mathop{\to} \limits_{r=1}^{a_2} S_9
$$

#### **return：** 沿着trajectory把所有rewards加起来

$$
return = 0 + 0 + 0 + 1 = 1
$$


### **Discounted return:**

#### discount rate: 折扣率 $\gamma \in [0, 1)$

#### Discounted return:

平衡更远未来所获得的reward


### **Episode：** 当agent按照策略与环境交互时，可能会在某些终端状态停止，这个结果trajectory称为一个episode


### **Markov decision process(MDP)**

#### Key elements of MDP:

- **Sets:**

  - State: the set of states $S$
  - Action: the set of actions $A(s)$ is associated for state $s \in S$
  - Reward: the set of rewards $R(s,a)$.

- **Probability distribution:**

  - State transition probability: 当前状态s下，执行行动a，转移到状态$s'$ 的概率 $p(p'|s, a)$
  - Reward probability:当前状态s下，执行行动a，获得奖励r的概率 $p(r|s, a)$

- **Policy：** 在状态s， 选择行动a的概率 $\pi(a|s)$

- **Markov property:**  memoryless property 无记忆
  $$
  p(s_{t+1}|a_{t+1}, s_t, \ldots ,a_1, s_0) = p(s_{t+1}|a_{t+1}, s_t) \\
  p(r_{t+1}|a_{t+1}, s_t, \ldots ,a_1, s_0) = p(r_{t+1}|a_{t+1}, s_t)
  $$
